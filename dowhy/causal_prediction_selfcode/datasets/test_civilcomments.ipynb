{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds.datasets.civilcomments_dataset import CivilCommentsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\04\\thesis\\dowhy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from civilcomments import DWCivilCommentsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\04\\thesis\\dowhy\\dowhy\\causal_prediction_selfcode\\datasets\\civilcomments.py:75: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  self._identity_array = torch.LongTensor(self._identity_array)\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "dataset = DWCivilCommentsDataset(root_dir=data_dir, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "class MyDistilBertTokenizer(DistilBertTokenizerFast):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if 'padding' not in kwargs:\n",
    "            kwargs['padding'] = 'max_length'\n",
    "        if 'max_length' not in kwargs:\n",
    "            kwargs['max_length'] = 300\n",
    "        if 'truncation' not in kwargs:\n",
    "            kwargs['truncation'] = True\n",
    "        if 'return_tensors' not in kwargs:\n",
    "            kwargs['return_tensors'] = 'pt'\n",
    "\n",
    "        tokens = super().__call__(*args, **kwargs)\n",
    "\n",
    "        x = torch.stack((tokens[\"input_ids\"], tokens[\"attention_mask\"]), dim=2)\n",
    "        x = torch.squeeze(x, dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupSampler:\n",
    "    \"\"\"\n",
    "        Constructs batches by first sampling groups,\n",
    "        then sampling data from those groups.\n",
    "        It drops the last batch if it's incomplete.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_ids, batch_size, n_groups_per_batch,\n",
    "                 uniform_over_groups, distinct_groups):\n",
    "\n",
    "        if batch_size % n_groups_per_batch != 0:\n",
    "            raise ValueError(f'batch_size ({batch_size}) must be evenly divisible by n_groups_per_batch ({n_groups_per_batch}).')\n",
    "        if len(group_ids) < batch_size:\n",
    "            raise ValueError(f'The dataset has only {len(group_ids)} examples but the batch size is {batch_size}. There must be enough examples to form at least one complete batch.')\n",
    "\n",
    "        self.group_ids = group_ids\n",
    "        self.unique_groups, self.group_indices, unique_counts = split_into_groups(group_ids)\n",
    "\n",
    "        self.distinct_groups = distinct_groups\n",
    "        self.n_groups_per_batch = n_groups_per_batch\n",
    "        self.n_points_per_group = batch_size // n_groups_per_batch\n",
    "\n",
    "        self.dataset_size = len(group_ids)\n",
    "        self.num_batches = self.dataset_size // batch_size\n",
    "\n",
    "        if uniform_over_groups: # Sample uniformly over groups\n",
    "            self.group_prob = None\n",
    "        else: # Sample a group proportionately to its size\n",
    "            self.group_prob = unique_counts.numpy() / unique_counts.numpy().sum()\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch_id in range(self.num_batches):\n",
    "            # Note that we are selecting group indices rather than groups\n",
    "            groups_for_batch = np.random.choice(\n",
    "                len(self.unique_groups),\n",
    "                size=self.n_groups_per_batch,\n",
    "                replace=(not self.distinct_groups),\n",
    "                p=self.group_prob)\n",
    "            sampled_ids = [\n",
    "                np.random.choice(\n",
    "                    self.group_indices[group],\n",
    "                    size=self.n_points_per_group,\n",
    "                    replace=len(self.group_indices[group]) <= self.n_points_per_group, # False if the group is larger than the sample size\n",
    "                    p=None)\n",
    "                for group in groups_for_batch]\n",
    "\n",
    "            # Flatten\n",
    "            sampled_ids = np.concatenate(sampled_ids)\n",
    "            yield sampled_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "NUM_WORKERS = 3\n",
    "\n",
    "\n",
    "def get_train_loader(loader, dataset, batch_size,\n",
    "        uniform_over_groups=None, grouper=None, distinct_groups=True, n_groups_per_batch=None, **loader_kwargs):\n",
    "    \"\"\"\n",
    "    Constructs and returns the data loader for training.\n",
    "    Args:\n",
    "        - loader (str): Loader type. 'standard' for standard loaders and 'group' for group loaders,\n",
    "                        which first samples groups and then samples a fixed number of examples belonging\n",
    "                        to each group.\n",
    "        - dataset (WILDSDataset or WILDSSubset): Data\n",
    "        - batch_size (int): Batch size\n",
    "        - uniform_over_groups (None or bool): Whether to sample the groups uniformly or according\n",
    "                                              to the natural data distribution.\n",
    "                                              Setting to None applies the defaults for each type of loaders.\n",
    "                                              For standard loaders, the default is False. For group loaders,\n",
    "                                              the default is True.\n",
    "        - grouper (Grouper): Grouper used for group loaders or for uniform_over_groups=True\n",
    "        - distinct_groups (bool): Whether to sample distinct_groups within each minibatch for group loaders.\n",
    "        - n_groups_per_batch (int): Number of groups to sample in each minibatch for group loaders.\n",
    "        - loader_kwargs: kwargs passed into torch DataLoader initialization.\n",
    "    Output:\n",
    "        - data loader (DataLoader): Data loader.\n",
    "    \"\"\"\n",
    "    if loader == 'standard':\n",
    "        if uniform_over_groups is None or not uniform_over_groups:\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                shuffle=True, # Shuffle training dataset\n",
    "                sampler=None,\n",
    "                collate_fn=dataset.collate,\n",
    "                batch_size=batch_size,\n",
    "                **loader_kwargs)\n",
    "        else:\n",
    "            assert grouper is not None\n",
    "            groups, group_counts = grouper.metadata_to_group(\n",
    "                dataset.metadata_array,\n",
    "                return_counts=True)\n",
    "            group_weights = 1 / group_counts\n",
    "            weights = group_weights[groups]\n",
    "\n",
    "            # Replacement needs to be set to True, otherwise we'll run out of minority samples\n",
    "            sampler = WeightedRandomSampler(weights, len(dataset), replacement=True)\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                shuffle=False, # The WeightedRandomSampler already shuffles\n",
    "                sampler=sampler,\n",
    "                collate_fn=dataset.collate,\n",
    "                batch_size=batch_size,\n",
    "                **loader_kwargs)\n",
    "\n",
    "    elif loader == 'group':\n",
    "        if uniform_over_groups is None:\n",
    "            uniform_over_groups = True\n",
    "        assert grouper is not None\n",
    "        assert n_groups_per_batch is not None\n",
    "        if n_groups_per_batch > grouper.n_groups:\n",
    "            raise ValueError(f'n_groups_per_batch was set to {n_groups_per_batch} but there are only {grouper.n_groups} groups specified.')\n",
    "\n",
    "        group_ids = grouper.metadata_to_group(dataset.metadata_array)\n",
    "        batch_sampler = GroupSampler(\n",
    "            group_ids=group_ids,\n",
    "            batch_size=batch_size,\n",
    "            n_groups_per_batch=n_groups_per_batch,\n",
    "            uniform_over_groups=uniform_over_groups,\n",
    "            distinct_groups=distinct_groups)\n",
    "\n",
    "        return DataLoader(dataset,\n",
    "              num_workers=NUM_WORKERS,\n",
    "              shuffle=None,\n",
    "              sampler=None,\n",
    "              collate_fn=dataset.collate,\n",
    "              batch_sampler=batch_sampler,\n",
    "              drop_last=False,\n",
    "              **loader_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'MyDistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Load the modified tokenizer\n",
    "tokenizer = MyDistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds.common.utils import split_into_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.get_subset('train', transform=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309220\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader('group', train_dataset, batch_size=16, uniform_over_groups=False, grouper=dataset.train_grouper, n_groups_per_batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_dir = 'data/civilcomments_v1.0'\n",
    "_metadata_df = pd.read_csv(\n",
    "            os.path.join(_data_dir, 'all_data_with_identities.csv'),\n",
    "            index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = _metadata_df.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_split_array = _metadata_df['split'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(_split_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
